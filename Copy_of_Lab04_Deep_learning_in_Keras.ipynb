{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MagixIsAvailable/NLP_labs/blob/main/Copy_of_Lab04_Deep_learning_in_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuCKrCMIYPJh"
      },
      "source": [
        "#Week 4 - Deep Learning\n",
        "\n",
        "In this lab we will be introducing how to implement deep learning algorithms in Keras. In particular, we are going to focus on CNN and RNN architectures.\n",
        "\n",
        "<br>\n",
        "\n",
        "Frameworks:\n",
        "\n",
        "Tensorflow - https://www.tensorflow.org/\n",
        "\n",
        "Keras - https://keras.io/  \n",
        "\n",
        "<br>\n",
        "\n",
        "We will be looking at two core libraries in this lab, Tensorflow and Keras. Tensorflow is a large library which empowers many different types of machine learning algorithms, but is perhaps more popularly known as creating efficient data structures to underpin the learning of weights and biases in a neural network. Keras builds on top of Tensorflow and makes it much easier to read and to work with. As a result, within the last few years Tensorflow has integrated some of the Keras functionality within their library (though an external Keras library still exists). While you can examine the documentation for a more flexhed out answer, the easiest way to remember the difference between the two is that Tensorflow gives you access to the functionality while Keras makes it easier to read and code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpnBcNU1cnjy"
      },
      "source": [
        "##Task 1 - Convolutional Neural Networks\n",
        "\n",
        "Now let us examine how we can implement a deep learning algorithm on a simple problem.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GxsLhzuLeu2z"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  WARNING: The script tensorboard.exe is installed in 'C:\\Users\\2224883\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
            "  WARNING: The scripts import_pb_to_tensorboard.exe, saved_model_cli.exe, tensorboard.exe, tf_upgrade_v2.exe, tflite_convert.exe and toco.exe are installed in 'C:\\Users\\2224883\\AppData\\Roaming\\Python\\Python313\\Scripts' which is not on PATH.\n",
            "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting tensorflow\n",
            "  Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl.metadata (4.6 kB)\n",
            "Collecting absl-py>=1.0.0 (from tensorflow)\n",
            "  Downloading absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting astunparse>=1.6.0 (from tensorflow)\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
            "  Downloading flatbuffers-25.9.23-py2.py3-none-any.whl.metadata (875 bytes)\n",
            "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
            "  Downloading gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Collecting google_pasta>=0.1.1 (from tensorflow)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
            "Collecting libclang>=13.0.0 (from tensorflow)\n",
            "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
            "Collecting opt_einsum>=2.3.2 (from tensorflow)\n",
            "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf>=5.28.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (5.29.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (72.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Collecting termcolor>=1.1.0 (from tensorflow)\n",
            "  Downloading termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: typing_extensions>=3.6.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.17.0)\n",
            "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
            "  Downloading grpcio-1.75.1-cp313-cp313-win_amd64.whl.metadata (3.8 kB)\n",
            "Collecting tensorboard~=2.20.0 (from tensorflow)\n",
            "  Downloading tensorboard-2.20.0-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting keras>=3.10.0 (from tensorflow)\n",
            "  Downloading keras-3.11.3-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.1.3)\n",
            "Requirement already satisfied: h5py>=3.11.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.12.1)\n",
            "Collecting ml_dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
            "  Downloading ml_dtypes-0.5.3-cp313-cp313-win_amd64.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (11.1.0)\n",
            "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.20.0->tensorflow)\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard~=2.20.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in c:\\programdata\\anaconda3\\lib\\site-packages (from keras>=3.10.0->tensorflow) (13.9.4)\n",
            "Collecting namex (from keras>=3.10.0->tensorflow)\n",
            "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
            "Collecting optree (from keras>=3.10.0->tensorflow)\n",
            "  Downloading optree-0.17.0-cp313-cp313-win_amd64.whl.metadata (34 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.20.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from rich->keras>=3.10.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.10.0->tensorflow) (0.1.0)\n",
            "Downloading tensorflow-2.20.0-cp313-cp313-win_amd64.whl (332.0 MB)\n",
            "   ---------------------------------------- 0.0/332.0 MB ? eta -:--:--\n",
            "    --------------------------------------- 8.1/332.0 MB 48.5 MB/s eta 0:00:07\n",
            "   -- ------------------------------------- 19.1/332.0 MB 51.7 MB/s eta 0:00:07\n",
            "   --- ------------------------------------ 31.5/332.0 MB 53.2 MB/s eta 0:00:06\n",
            "   ----- ---------------------------------- 43.8/332.0 MB 54.3 MB/s eta 0:00:06\n",
            "   ------ --------------------------------- 55.6/332.0 MB 55.2 MB/s eta 0:00:06\n",
            "   -------- ------------------------------- 67.6/332.0 MB 55.9 MB/s eta 0:00:05\n",
            "   --------- ------------------------------ 80.7/332.0 MB 56.6 MB/s eta 0:00:05\n",
            "   ----------- ---------------------------- 92.8/332.0 MB 57.1 MB/s eta 0:00:05\n",
            "   ------------ -------------------------- 106.2/332.0 MB 57.6 MB/s eta 0:00:04\n",
            "   ------------- ------------------------- 118.5/332.0 MB 58.1 MB/s eta 0:00:04\n",
            "   --------------- ----------------------- 131.3/332.0 MB 58.6 MB/s eta 0:00:04\n",
            "   ----------------- --------------------- 145.2/332.0 MB 59.1 MB/s eta 0:00:04\n",
            "   ------------------ -------------------- 158.3/332.0 MB 59.5 MB/s eta 0:00:03\n",
            "   -------------------- ------------------ 172.2/332.0 MB 59.9 MB/s eta 0:00:03\n",
            "   --------------------- ----------------- 185.1/332.0 MB 60.2 MB/s eta 0:00:03\n",
            "   ----------------------- --------------- 197.9/332.0 MB 60.4 MB/s eta 0:00:03\n",
            "   ------------------------ -------------- 212.1/332.0 MB 60.7 MB/s eta 0:00:02\n",
            "   -------------------------- ------------ 225.2/332.0 MB 61.0 MB/s eta 0:00:02\n",
            "   ---------------------------- ---------- 239.6/332.0 MB 61.3 MB/s eta 0:00:02\n",
            "   ----------------------------- --------- 253.5/332.0 MB 61.6 MB/s eta 0:00:02\n",
            "   ------------------------------- ------- 267.6/332.0 MB 62.7 MB/s eta 0:00:02\n",
            "   --------------------------------- ----- 282.6/332.0 MB 63.3 MB/s eta 0:00:01\n",
            "   ---------------------------------- ---- 297.8/332.0 MB 64.1 MB/s eta 0:00:01\n",
            "   ------------------------------------ -- 313.0/332.0 MB 64.7 MB/s eta 0:00:01\n",
            "   --------------------------------------  328.2/332.0 MB 65.3 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------  331.9/332.0 MB 65.5 MB/s eta 0:00:01\n",
            "   --------------------------------------- 332.0/332.0 MB 22.0 MB/s eta 0:00:00\n",
            "Downloading grpcio-1.75.1-cp313-cp313-win_amd64.whl (4.6 MB)\n",
            "   ---------------------------------------- 0.0/4.6 MB ? eta -:--:--\n",
            "   ---------------------------------------- 4.6/4.6 MB 28.7 MB/s eta 0:00:00\n",
            "Downloading ml_dtypes-0.5.3-cp313-cp313-win_amd64.whl (208 kB)\n",
            "Downloading tensorboard-2.20.0-py3-none-any.whl (5.5 MB)\n",
            "   ---------------------------------------- 0.0/5.5 MB ? eta -:--:--\n",
            "   ---------------------------------------- 5.5/5.5 MB 28.7 MB/s eta 0:00:00\n",
            "Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
            "Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
            "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Downloading flatbuffers-25.9.23-py2.py3-none-any.whl (30 kB)\n",
            "Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
            "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "Downloading keras-3.11.3-py3-none-any.whl (1.4 MB)\n",
            "   ---------------------------------------- 0.0/1.4 MB ? eta -:--:--\n",
            "   ---------------------------------------- 1.4/1.4 MB 15.6 MB/s eta 0:00:00\n",
            "Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
            "   ---------------------------------------- 0.0/26.4 MB ? eta -:--:--\n",
            "   ---------------------- ----------------- 14.7/26.4 MB 73.5 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 73.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 73.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 73.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------  26.2/26.4 MB 73.1 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 26.4/26.4 MB 25.2 MB/s eta 0:00:00\n",
            "Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
            "Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
            "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
            "Downloading optree-0.17.0-cp313-cp313-win_amd64.whl (316 kB)\n",
            "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt_einsum, ml_dtypes, grpcio, google_pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow\n",
            "\n",
            "   -- -------------------------------------  1/16 [libclang]\n",
            "   -- -------------------------------------  1/16 [libclang]\n",
            "   ----- ----------------------------------  2/16 [flatbuffers]\n",
            "   ---------- -----------------------------  4/16 [tensorboard-data-server]\n",
            "   ------------ ---------------------------  5/16 [optree]\n",
            "   ------------ ---------------------------  5/16 [optree]\n",
            "   --------------- ------------------------  6/16 [opt_einsum]\n",
            "   --------------- ------------------------  6/16 [opt_einsum]\n",
            "   -------------------- -------------------  8/16 [grpcio]\n",
            "   -------------------- -------------------  8/16 [grpcio]\n",
            "   -------------------- -------------------  8/16 [grpcio]\n",
            "   -------------------- -------------------  8/16 [grpcio]\n",
            "   -------------------- -------------------  8/16 [grpcio]\n",
            "   ---------------------- -----------------  9/16 [google_pasta]\n",
            "   ---------------------- -----------------  9/16 [google_pasta]\n",
            "   ------------------------- -------------- 10/16 [gast]\n",
            "   --------------------------- ------------ 11/16 [astunparse]\n",
            "   ------------------------------ --------- 12/16 [absl-py]\n",
            "   ------------------------------ --------- 12/16 [absl-py]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   -------------------------------- ------- 13/16 [tensorboard]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ----------------------------------- ---- 14/16 [keras]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ------------------------------------- -- 15/16 [tensorflow]\n",
            "   ---------------------------------------- 16/16 [tensorflow]\n",
            "\n",
            "Successfully installed absl-py-2.3.1 astunparse-1.6.3 flatbuffers-25.9.23 gast-0.6.0 google_pasta-0.2.0 grpcio-1.75.1 keras-3.11.3 libclang-18.1.1 ml_dtypes-0.5.3 namex-0.1.0 opt_einsum-3.4.0 optree-0.17.0 tensorboard-2.20.0 tensorboard-data-server-0.7.2 tensorflow-2.20.0 termcolor-3.1.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install tensorflow\n",
        "\n",
        "import numpy as np #For the array functions\n",
        "from tensorflow import keras #we will only be using tensorflow as a backend, so we will import keras via their implementation\n",
        "from tensorflow.keras import layers #we will use the readable keras implementation of the conv/reccurent layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EYXfVflnFac_"
      },
      "source": [
        "Next let's load the dataset. While we could use any of the datasets we have examined in the module thus far, Keras also has some datasets stored within the library. This means they can be accessed quickly and efficiently. This also allows us to load the training and test sets directly using a split identified by the dataset designer.\n",
        "\n",
        "\n",
        "Let's load the MNIST dataset. MNIST is a handwriting recognition task where the goal is to separate images into categories 0 to 9 (depending on the subject of the image)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nK9Q5PeDCeNl",
        "outputId": "2169f1a8-e5d8-4ec0-d368-4717999a25d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "(x_train, y_train),(x_test, y_test) = keras.datasets.mnist.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "yxPQN5l0Gry6",
        "outputId": "13678d90-caf3-41e7-fcb8-d6e2a82f2de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#We can view the size and dimensions of each array by calling the shape attribute\n",
        "print(x_train.shape)\n",
        "#Calling the above will demonstrate we have 60,000 images in our training set, each of which are square images of 28 pixels high and wide\n",
        "\n",
        "#We can view one of the images to see what it looks like\n",
        "from PIL import Image #we import the pillow libary to convert arrays into images\n",
        "import matplotlib.pyplot as plt #then import the matplotlib library to view the image in our code\n",
        "\n",
        "img = Image.fromarray(x_train[0]) #we extract the image we want to view by taking a random image from the training set and using the fromArray() function to construct an image from pixel information\n",
        "imgplot = plt.imshow(img) #we then plot the image\n",
        "plt.show() #and show it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vGd6yS1TznwC"
      },
      "outputs": [],
      "source": [
        "#Next we will preprocess the dataset\n",
        "\n",
        "#To start we will normalise the contents of the arrays. We will use min max normalisation, where we know the maximum value is 255\n",
        "#This will scale images to the [0, 1] range\n",
        "x_train = x_train.astype(\"float32\") / 255\n",
        "x_test = x_test.astype(\"float32\") / 255\n",
        "\n",
        "#The next setps require us to identify the number of classes - in our problem we have 10\n",
        "num_classes = 10\n",
        "\n",
        "#Then we convert class vectors to one hot vectors - i.e. vectors of the size of number of classes, with a 1 in the class index and a 0 in every other class index\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOMXzT0rFYfa"
      },
      "source": [
        "Now that we have our dataset, it is time to configure our CNN architecture.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "JQ96zZAWnNKb",
        "outputId": "d722f8a2-b552-4ad6-a2ae-2f3a1caf3515"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)          │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)          │          <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1600</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,010</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m26\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │             \u001b[38;5;34m320\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m32\u001b[0m)          │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m64\u001b[0m)          │          \u001b[38;5;34m18,496\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1600\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │          \u001b[38;5;34m16,010\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,826</span> (136.04 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,826\u001b[0m (136.04 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = keras.Sequential( #first we create a model and specify we are using the Sequential configuration within Keras\n",
        "    [\n",
        "        keras.Input(shape=(28,28,1)), #Next we specify the size of our input layer. For MNIST, this will be 28 pixels by 28 pixels. The 1 indicates there is only a single colour dimension (black and white)\n",
        "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"), #Next we add a convolution layer.  We state there will be 32 filters, and each filter will be size 3 x 3\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)), #After the convolution layer we add a MP layer. The MP layer will summarise 2x2 portions of our activation matrix from our convolution layer\n",
        "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"), #we can repeat as often as necessary\n",
        "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "        layers.Flatten(), #Eventually, we flatten the output of the last MP layer, so that it becomes a vector instead of a matrix\n",
        "        layers.Dense(num_classes, activation=\"softmax\"), #We then send to our outpt layyer (which has 10 neurons, one for each class) and a softmax activation function\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzNa8YquV8-0"
      },
      "source": [
        "We can then train and test the model and see how it performs on our data.\n",
        "\n",
        "Warning, the below cell will take some time to complete (approx 5 - 10 minutes, or more if you increase the number of epochs)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24wYRVsyWGk1",
        "outputId": "6325b82c-e22a-42f3-b79f-69b5dcd42510"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 119ms/step - accuracy: 0.4081 - loss: 1.9649\n",
            "Epoch 2/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 106ms/step - accuracy: 0.8826 - loss: 0.4058\n",
            "Epoch 3/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 104ms/step - accuracy: 0.9142 - loss: 0.2930\n",
            "Epoch 4/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 97ms/step - accuracy: 0.9303 - loss: 0.2394\n",
            "Epoch 5/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 97ms/step - accuracy: 0.9393 - loss: 0.2028\n",
            "Epoch 6/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 101ms/step - accuracy: 0.9492 - loss: 0.1748\n",
            "Epoch 7/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 99ms/step - accuracy: 0.9565 - loss: 0.1518\n",
            "Epoch 8/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 104ms/step - accuracy: 0.9612 - loss: 0.1346\n",
            "Epoch 9/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 100ms/step - accuracy: 0.9642 - loss: 0.1245\n",
            "Epoch 10/10\n",
            "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 98ms/step - accuracy: 0.9688 - loss: 0.1094\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ce16928c1c0>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 128 #the batch size is the number of examples the model will view before summing and backpropagating the loss\n",
        "epochs = 10 #epochs are the number of training iterations examining the full training set\n",
        "\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=[\"accuracy\"]) #We can set the model to compile. We call some inbuilt functions for our loss, optimizer and metrics respectively. Can you identify them from the lectures?\n",
        "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs) #Finally, we train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjbCuwjyW3kc",
        "outputId": "7c0d17c4-9ad1-4de3-e36a-67464d616523"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.09826838225126266\n",
            "Test accuracy: 0.972100019454956\n"
          ]
        }
      ],
      "source": [
        "#We can then test our model on the test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0) #the model evaluate function contains some useful metrics to evaluate our model. We set verbose to 0 so we can print the ones we are interested in\n",
        "print(\"Test loss:\", score[0]) #Let's look at loss\n",
        "print(\"Test accuracy:\", score[1]) #And accuracy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wxTrtvtjzoOH"
      },
      "source": [
        "And there we have it, a functioning CNN on the MNIST dataset. Make sure to read over the code carefully and when you are ready attempt the exercises below.\n",
        "\n",
        "### Task 1.1 - Exploring the Code\n",
        "1.   What happens if you add an additional dense layer before the output layer? Why do you think this is the case?\n",
        "2.   Try adjusting some of the hyperparameters in the code - for example, the number of convolutional filters, activations, etc. What effect does this have?\n",
        "\n",
        "<br>\n",
        "\n",
        "### Task 1.2 - Try it Out on Something New\n",
        "Now you are confident in working with the code, try expanding the code to cover the CIFAR-10 dataset. This dataset is also available in Keras, so by exploring the documentation you should be able to load it in the same way as MNIST. However, the dataset preprocessing and the CNN architecture will need some adjustments to fit the new data.\n",
        "\n",
        "<br>\n",
        "\n",
        "### Task 1.3 - Extra\n",
        "What changes would you have to make to the MNIST code so that you could use an MLP rather than a CNN model? Try exploring the documentation for ideas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RcqT6DELzrSb"
      },
      "source": [
        "##Task 2 - Recurrent Neural Networks\n",
        "Next let's have a look at how we could implement an RNN.\n",
        "\n",
        "For this we will examine a many-to-one task - i.e. we have multiple sequential inputs, but we are only interested in generating a single label.\n",
        "\n",
        "The task we will focus on is sentiment classification - a type of text classification where we want to allocate a sentiment category to a sentence. This is commonly used in areas like reviews or analysing feedback from social media posts.\n",
        "\n",
        "The specific dataset we will look at today is the IMDB dataset. This is a collection of movie reviews from IMDB, and the goal is to identify whether the reviews were positive or negative just by reading the text. There are going to be some concepts for processing text which may be a little unfamiliar, but don't worry, we cover them next week."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJsFuzh02cwA"
      },
      "outputs": [],
      "source": [
        "import numpy as np #For the array functions\n",
        "from tensorflow import keras #we will only be using tensorflow as a backend, so we will import keras via their implementation\n",
        "from tensorflow.keras import layers #we will use the readable keras implementation of the conv/reccurent layers\n",
        "from keras.datasets import imdb #start by loading the dataset\n",
        "\n",
        "#there's a lot of variation in weights, so let's fix random seed for reproducibility\n",
        "np.random.seed(1337)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ca6w96GruBgf",
        "outputId": "d6144282-701a-4ff1-fd02-8268b35d5982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "#Then we want to load the dataset but only keep the top n words (ranked by how often they occur in the training set), and ignore less common words\n",
        "top_words = 5000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=top_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PsrKdn1UuIc3",
        "outputId": "d56f73da-be5b-4df5-e258-ad78de26ddbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(25000,)\n",
            "(25000,)\n",
            "{0, 1}\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ]
        }
      ],
      "source": [
        "#We can view the size and dimensions of each array by calling the shape attribute\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n",
        "#Calling the above will demonstrate we have 25,000 reviews in our training and test sets\n",
        "\n",
        "#Next, let's look at the unique labels in our dataset\n",
        "print(set(y_train)) #casting as a set is a quick way we can identify the unique values in an array\n",
        "#In this case we have two values - 1 and 0, corresponding to positive and negative. This is therefore a binary task\n",
        "\n",
        "#Finally, let's have a look at an actual review\n",
        "print(x_train[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo6m6UC4Gspm"
      },
      "source": [
        "Uh oh, that doesn't quite look right - these are meant to be textual reviews, so what happened?\n",
        "\n",
        "Well, to make the datasets smaller and easier to store, Keras has pre-emptively converted each word in the vocabulary to a number.\n",
        "\n",
        "This number stores the index where the representation of the word exists (more on this over the next few week, but essentially each word is represented by its own vector)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XUUFVN_uGzxH",
        "outputId": "262060de-0dd6-4c79-8c63-d09c2f4aaae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "the as you with out themselves powerful lets loves their becomes reaching had journalist of lot from anyone to have after out atmosphere never more room and it so heart shows to years of every never going and help moments or of every chest visual movie except her was several of enough more with is now current film as you of mine potentially unfortunately of you than him that with out themselves her get for was camp of you movie sometimes movie that with scary but and to story wonderful that in seeing in character to of 70s and with heart had shadows they of here that with her serious to have does when from why what have critics they is you that isn't one will very to as itself with other and in of seen over and for anyone of and br show's to whether from than out themselves history he name half some br of and odd was two most of mean for 1 any an boat she he should is thought and but of script you not while history he heart to real at and but when from one bit then have two of script their with her nobody most that with wasn't to with armed acting watch an for with and film want an\n"
          ]
        }
      ],
      "source": [
        "#To read the input to the network, we have to convert each index to its corresponding word\n",
        "word_index = keras.datasets.imdb.get_word_index() #first we get the index list\n",
        "inverted_word_index = dict((i, word) for (word, i) in word_index.items()) #then we create a local dictionary, storing each index and its corresponding word\n",
        "print(\" \".join(inverted_word_index[i] for i in x_train[0])) #finally, we can convert the review to text by iterating an example through the dictionary\n",
        "#Keep in mind that we have only kept the 5,000 most common words. As a result, the sentence we print may be difficult to read.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3eOhc74Jwgd9"
      },
      "source": [
        "Pad the Input\n",
        "\n",
        "Next, we need to truncate and pad the input sequences so that they are all the same length for modeling. The model will learn the zero values carry no information so indeed the sequences are not the same length in terms of content, but same length vectors is required to perform the computation in Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzMe4iRYwp0f"
      },
      "outputs": [],
      "source": [
        "from keras.preprocessing import sequence #import the relevant function\n",
        "\n",
        "# truncate and pad input sequences\n",
        "max_review_length = 500\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=max_review_length)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=max_review_length)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yiejqZjPw59-"
      },
      "source": [
        "Now we have finished preprocessing the dataset, we can move on to setting up our RNN architecture."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        },
        "id": "6OosqqrbxFUk",
        "outputId": "376eb8d2-167b-44f2-b500-327325cbbbee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">SimpleRNN</span>)               │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ simple_rnn (\u001b[38;5;33mSimpleRNN\u001b[0m)               │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from keras.layers import Embedding #first import the embedding layer. This is needed internally for Keras to convert the text indices to their representations\n",
        "\n",
        "model = keras.Sequential( #first we create a model and specify we are using the Sequential configuration within Keras\n",
        "    [\n",
        "        Embedding(input_dim=top_words, output_dim=32, input_length=max_review_length), #We set up this layer using some of our parameters from earlier.\n",
        "        layers.SimpleRNN(32, activation=\"relu\"), #we then add a simpleRNN layer. Keras uses simple to mean the standard RNN, as much research is moving on to LSTM or GRU layers now.\n",
        "        layers.Dense(1, activation='sigmoid') #Then we pass to out output. This layer has only a single neuron and a sigmoid activation function, as it will give either a 1 or 0 as output (facilitating binary classification)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72J9zXE_yQE-",
        "outputId": "3dff3c97-c92a-4627-abb5-22657d556c82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 130ms/step - accuracy: 0.5985 - loss: 0.6438\n",
            "Epoch 2/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 136ms/step - accuracy: 0.8149 - loss: 0.4247\n",
            "Epoch 3/3\n",
            "\u001b[1m391/391\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m81s\u001b[0m 134ms/step - accuracy: 0.8165 - loss: 0.4060\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ce156bc5ed0>"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Finally, as before we can train the model\n",
        "epochs = 3\n",
        "batch_size = 64\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size) #similar to the CNN above, we can set our parameters and compile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rR-PQ3y0oqK",
        "outputId": "c2c0e296-c23c-4978-b52e-ab6025738980"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test loss: 0.5236312747001648\n",
            "Test accuracy: 0.7271199822425842\n"
          ]
        }
      ],
      "source": [
        "#We can then test our model on the test set\n",
        "score = model.evaluate(x_test, y_test, verbose=0) #the model evaluate function contains some useful metrics to evaluate our model. We set verbose to 0 so we can print the ones we are interested in\n",
        "print(\"Test loss:\", score[0]) #Let's look at loss\n",
        "print(\"Test accuracy:\", score[1]) #And accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tSmemHAj1RyX"
      },
      "source": [
        "Voila, a functioning RNN on the IMDB dataset. Make sure to read over the code carefully and when you are ready attempt the exercises below.\n",
        "\n",
        "### Task 2.1 - Exploring the Code\n",
        "1.   What happens if you add an additional aimple RNN layer before the output layer? Why do you think this is the case?\n",
        "2.   Try adjusting some of the hyperparameters in the code - for example, the optimizer, number of epochs, etc. What effect does this have?\n",
        "\n",
        "<br>\n",
        "\n",
        "### Task 2.2 - Try it Out on Something New\n",
        "As with the CNN, it's a good idea to get comfortable applying new datasets. With that in mind, try writing the code to apply this to the Reuters Newswire dataset (or do feel free to select a dataset from UCI).\n",
        "\n",
        "<br>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "hpnBcNU1cnjy"
      ],
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
